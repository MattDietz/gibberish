package markov

import (
	"bufio"
	"encoding/json"
	"fmt"
	"log"
	"math"
	"math/rand"
	"os"
	"strings"
)

const (
	ENTROPY_THRESHOLD = 4.0

	NGRAM_CHAR TrainType = iota
	NGRAM_WORD
)

type NGramMatrix map[string]map[string]float64
type TrainType int

type Model struct {
	Threshold    float64     `json:"threshold"`
	Probs        NGramMatrix `json:"probs"`
	UniqueNGrams int         `json:"unique_ngrams"`
}

func LoadModel(path string) *Model {
	m := &Model{}

	f, err := os.Open(path)
	if err != nil {
		log.Fatal(err)
	}
	defer f.Close()
	decoder := json.NewDecoder(f)
	decoder.Decode(m)
	return m
}

func generateNgrams(input string, ngramSize int) []string {
	var result []string
	input = strings.ToLower(input)
	for i := 0; i < len(input)-ngramSize+1; i++ {
		for j := 0; j < ngramSize; j++ {
			if !validChar(rune(input[i+j])) {
				continue
			}
		}
		result = append(result, input[i:i+ngramSize])
	}
	return result
}

func validChar(r rune) bool {
	return r >= 32 && r <= 126
}

func Train(trainPath, goodPath, badPath string, ngramSize int) *Model {
	file, err := os.Open(trainPath)
	if err != nil {
		log.Fatal(err)
	}

	counts := make(NGramMatrix)
	contextCounts := make(map[string]int)

	uniqueNgrams := 0
	scanner := bufio.NewScanner(file)
	padding := strings.Repeat(" ", 2)
	for scanner.Scan() {
		// Padding the string with whitespace helps generalize the probabilities for markov chains
		line := padding + strings.TrimSpace(scanner.Text()) + padding
		for _, ngram := range generateNgrams(line, ngramSize) {
			prevNgram := ngram[:ngramSize-1]
			nextChar := string(ngram[ngramSize-1])
			if _, ok := counts[prevNgram]; !ok {
				counts[prevNgram] = make(map[string]float64)
				uniqueNgrams++
			}
			counts[prevNgram][nextChar]++
			contextCounts[prevNgram]++
		}
	}

	// normalize
	probs := make(NGramMatrix)
	for prevNgram, nextCharCounts := range counts {
		probs[prevNgram] = make(map[string]float64)
		for char, count := range nextCharCounts {
			probs[prevNgram][char] = math.Log((count + 1) / (float64(contextCounts[prevNgram] + uniqueNgrams)))
		}
	}

	goodProbs := calculateProbabilities(goodPath, probs, uniqueNgrams)
	badProbs := calculateProbabilities(badPath, probs, uniqueNgrams)

	minGood := goodProbs[0]
	for _, prob := range goodProbs {
		if prob < minGood {
			minGood = prob
		}
	}

	maxBad := badProbs[0]
	for _, prob := range badProbs {
		if prob > maxBad {
			maxBad = prob
		}
	}

	threshold := (minGood + maxBad) / 2.0
	fmt.Println(threshold, minGood, maxBad, uniqueNgrams)
	return &Model{Threshold: threshold, Probs: probs, UniqueNGrams: uniqueNgrams}
}

func calculateProbabilities(path string, transitionProbs NGramMatrix, uniqueNgrams int) []float64 {
	file, err := os.Open(path)
	if err != nil {
		log.Fatal(err)
	}
	var outputProbs []float64
	scanner := bufio.NewScanner(file)
	for scanner.Scan() {
		line := strings.TrimSpace(scanner.Text())
		prob, err := averageTransitionProbability(line, transitionProbs, uniqueNgrams)
		if err != nil {
			continue
		}

		outputProbs = append(outputProbs, prob)
	}
	return outputProbs
}

func averageTransitionProbability(input string, probs NGramMatrix, uniqueNgrams int) (float64, error) {
	// Default probability for unseen trigrams
	logProb := 1.0 / float64(uniqueNgrams)
	// logProb := 0.0

	ngramSize := 0
	for k, _ := range probs {
		ngramSize = len(k) + 1
		break
	}

	for _, ngram := range generateNgrams(input, ngramSize) {
		prevNgram := ngram[:ngramSize-1]
		nextChar := string(ngram[ngramSize-1])
		if _, ok := probs[prevNgram]; ok {
			// The probabilities are stored as log probabilities in the model to avoid underflow
			if prob, ok := probs[prevNgram][nextChar]; ok {
				logProb += prob
			} else {
				return 0, fmt.Errorf("no probability for %v", ngram)
			}
		} else {
			return 0, fmt.Errorf("no probability for %v", ngram)
		}
	}

	return math.Exp(logProb), nil
}

func sumSlice(slice []float64) float64 {
	sum := 0.0
	for _, value := range slice {
		sum += value
	}
	return sum
}

func (m *Model) Save(path string) error {
	f, err := os.Create(path)
	if err != nil {
		log.Fatal(err)
	}
	defer f.Close()

	encoder := json.NewEncoder(f)
	return encoder.Encode(m)
}

// Tests if the given input string could be generated by this model
func (m *Model) Test(input string) (bool, error) {
	prob, err := averageTransitionProbability(input, m.Probs, m.UniqueNGrams)
	if err != nil {
		return false, err
	}
	return prob > m.Threshold, nil
}

func (m *Model) Probability(input string) (float64, error) {
	return averageTransitionProbability(input, m.Probs, m.UniqueNGrams)
}

func (m *Model) randomChoice(chars []string, probs []float64) string {
	cumulativeProb := 0.0
	sumProbs := sumSlice(probs)
	randomValue := rand.Float64() * sumProbs

	for i, char := range chars {
		cumulativeProb += probs[i]
		if randomValue < cumulativeProb {
			return char
		}
	}

	return chars[len(chars)-1]
}

func (m *Model) Generate(length int, seed string) string {
	currentBigram := seed
	var generatedText strings.Builder

	for i := 0; i < length; i++ {
		nextCharProbs, exists := m.Probs[currentBigram]
		if !exists {
			break
		}

		var chars []string
		var probs []float64
		for char, prob := range nextCharProbs {
			chars = append(chars, char)
			probs = append(probs, math.Exp(prob))
		}

		nextChar := m.randomChoice(chars, probs)
		generatedText.WriteString(nextChar)
		currentBigram = currentBigram[1:] + string(nextChar)
	}

	return generatedText.String()
}
